{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BrqO66YCU1P8"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "olawXwUdVD_8"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CZ32Ea7CVHPH"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD5YN33hVI_i",
        "outputId": "67d18545-226c-48ec-dd5e-fa2396f2ab30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading alzheimers-dataset-4-class-of-images.zip to /content\n",
            " 88% 30.0M/34.1M [00:00<00:00, 163MB/s]\n",
            "100% 34.1M/34.1M [00:00<00:00, 163MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d tourist55/alzheimers-dataset-4-class-of-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4E5D727hVKos"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/alzheimers-dataset-4-class-of-images.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klXH3kfYViIc",
        "outputId": "e5376515-33ee-4131-eccf-3a22f4b7c4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting imutils\n",
            "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: imutils\n",
            "  Building wheel for imutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25837 sha256=fb3db378c5cc56d70879e92fae611a8bc46035ce67bbf716d5d8d2fa5dd8297f\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\n",
            "Successfully built imutils\n",
            "Installing collected packages: imutils\n",
            "Successfully installed imutils-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AtiDDpwVNQo",
        "outputId": "563a1868-51fc-4712-b9c7-53a51394922f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 0 | global_acc: 49.318% | global_loss: 1.3186146020889282\n",
            "17/17 [==============================] - 1s 54ms/step\n",
            "comm_round: 1 | global_acc: 49.318% | global_loss: 1.2622722387313843\n",
            "17/17 [==============================] - 1s 54ms/step\n",
            "comm_round: 2 | global_acc: 51.072% | global_loss: 1.236676573753357\n",
            "17/17 [==============================] - 1s 55ms/step\n",
            "comm_round: 3 | global_acc: 53.996% | global_loss: 1.223749041557312\n",
            "17/17 [==============================] - 1s 54ms/step\n",
            "comm_round: 4 | global_acc: 52.827% | global_loss: 1.217298150062561\n",
            "17/17 [==============================] - 1s 54ms/step\n",
            "comm_round: 5 | global_acc: 54.971% | global_loss: 1.2275115251541138\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 6 | global_acc: 55.556% | global_loss: 1.2318998575210571\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 7 | global_acc: 55.361% | global_loss: 1.2267135381698608\n",
            "17/17 [==============================] - 1s 55ms/step\n",
            "comm_round: 8 | global_acc: 57.505% | global_loss: 1.202595591545105\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 9 | global_acc: 55.361% | global_loss: 1.2213010787963867\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 10 | global_acc: 54.776% | global_loss: 1.2305916547775269\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 11 | global_acc: 54.971% | global_loss: 1.2115330696105957\n",
            "17/17 [==============================] - 1s 55ms/step\n",
            "comm_round: 12 | global_acc: 54.971% | global_loss: 1.214281678199768\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 13 | global_acc: 57.700% | global_loss: 1.1971954107284546\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 14 | global_acc: 58.869% | global_loss: 1.2049294710159302\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 15 | global_acc: 57.505% | global_loss: 1.2146987915039062\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 16 | global_acc: 55.945% | global_loss: 1.2080535888671875\n",
            "17/17 [==============================] - 1s 55ms/step\n",
            "comm_round: 17 | global_acc: 58.674% | global_loss: 1.1916329860687256\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 18 | global_acc: 54.776% | global_loss: 1.2151618003845215\n",
            "17/17 [==============================] - 1s 54ms/step\n",
            "comm_round: 19 | global_acc: 57.895% | global_loss: 1.2033730745315552\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 20 | global_acc: 59.259% | global_loss: 1.1872276067733765\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 21 | global_acc: 58.480% | global_loss: 1.1851400136947632\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 22 | global_acc: 58.869% | global_loss: 1.1941460371017456\n",
            "17/17 [==============================] - 1s 54ms/step\n",
            "comm_round: 23 | global_acc: 55.945% | global_loss: 1.203426718711853\n",
            "17/17 [==============================] - 1s 55ms/step\n",
            "comm_round: 24 | global_acc: 59.259% | global_loss: 1.191871166229248\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 25 | global_acc: 58.674% | global_loss: 1.1913700103759766\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 26 | global_acc: 57.505% | global_loss: 1.1827671527862549\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 27 | global_acc: 59.454% | global_loss: 1.1956756114959717\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 28 | global_acc: 58.869% | global_loss: 1.176211953163147\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 29 | global_acc: 58.869% | global_loss: 1.1839663982391357\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 30 | global_acc: 56.530% | global_loss: 1.1894813776016235\n",
            "17/17 [==============================] - 1s 53ms/step\n",
            "comm_round: 31 | global_acc: 59.649% | global_loss: 1.1866142749786377\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 32 | global_acc: 58.285% | global_loss: 1.182497262954712\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 33 | global_acc: 57.700% | global_loss: 1.1905049085617065\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 34 | global_acc: 60.429% | global_loss: 1.170045256614685\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 35 | global_acc: 60.429% | global_loss: 1.1740914583206177\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 36 | global_acc: 61.209% | global_loss: 1.170190453529358\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 37 | global_acc: 59.649% | global_loss: 1.175574541091919\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 38 | global_acc: 59.259% | global_loss: 1.176663875579834\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 39 | global_acc: 59.259% | global_loss: 1.1706714630126953\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 40 | global_acc: 59.649% | global_loss: 1.182411551475525\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 41 | global_acc: 60.429% | global_loss: 1.1690164804458618\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 42 | global_acc: 60.039% | global_loss: 1.1706483364105225\n",
            "17/17 [==============================] - 2s 54ms/step\n",
            "comm_round: 43 | global_acc: 60.234% | global_loss: 1.166429042816162\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 44 | global_acc: 60.819% | global_loss: 1.16316819190979\n",
            "17/17 [==============================] - 2s 55ms/step\n",
            "comm_round: 45 | global_acc: 61.209% | global_loss: 1.169710397720337\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 46 | global_acc: 60.234% | global_loss: 1.1614412069320679\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 47 | global_acc: 60.039% | global_loss: 1.1670374870300293\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 48 | global_acc: 60.234% | global_loss: 1.157638430595398\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 49 | global_acc: 61.988% | global_loss: 1.1636772155761719\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 50 | global_acc: 60.429% | global_loss: 1.1578730344772339\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 51 | global_acc: 60.039% | global_loss: 1.1557790040969849\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 52 | global_acc: 60.819% | global_loss: 1.152052879333496\n",
            "17/17 [==============================] - 1s 55ms/step\n",
            "comm_round: 53 | global_acc: 61.404% | global_loss: 1.1669317483901978\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 54 | global_acc: 61.598% | global_loss: 1.1548103094100952\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 55 | global_acc: 63.353% | global_loss: 1.154407024383545\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 56 | global_acc: 62.378% | global_loss: 1.1532950401306152\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 57 | global_acc: 63.158% | global_loss: 1.1572248935699463\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 58 | global_acc: 61.793% | global_loss: 1.161763310432434\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 59 | global_acc: 61.598% | global_loss: 1.147294282913208\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 60 | global_acc: 62.573% | global_loss: 1.1570581197738647\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 61 | global_acc: 60.819% | global_loss: 1.1487425565719604\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 62 | global_acc: 63.158% | global_loss: 1.1458086967468262\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 63 | global_acc: 62.183% | global_loss: 1.1498992443084717\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 64 | global_acc: 62.963% | global_loss: 1.1517988443374634\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 65 | global_acc: 63.548% | global_loss: 1.1487749814987183\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 66 | global_acc: 62.573% | global_loss: 1.14230477809906\n",
            "17/17 [==============================] - 2s 61ms/step\n",
            "comm_round: 67 | global_acc: 62.963% | global_loss: 1.1421438455581665\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 68 | global_acc: 61.988% | global_loss: 1.1413716077804565\n",
            "17/17 [==============================] - 2s 61ms/step\n",
            "comm_round: 69 | global_acc: 64.327% | global_loss: 1.1441658735275269\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 70 | global_acc: 64.717% | global_loss: 1.1447173357009888\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 71 | global_acc: 64.912% | global_loss: 1.1537657976150513\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 72 | global_acc: 64.912% | global_loss: 1.1460204124450684\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 73 | global_acc: 63.743% | global_loss: 1.141165852546692\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 74 | global_acc: 65.107% | global_loss: 1.1423267126083374\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 75 | global_acc: 63.743% | global_loss: 1.1397624015808105\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 76 | global_acc: 64.133% | global_loss: 1.14128839969635\n",
            "17/17 [==============================] - 2s 62ms/step\n",
            "comm_round: 77 | global_acc: 63.743% | global_loss: 1.1347304582595825\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 78 | global_acc: 64.522% | global_loss: 1.1329337358474731\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 79 | global_acc: 65.497% | global_loss: 1.1432127952575684\n",
            "17/17 [==============================] - 2s 61ms/step\n",
            "comm_round: 80 | global_acc: 62.963% | global_loss: 1.1342800855636597\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 81 | global_acc: 66.667% | global_loss: 1.1303647756576538\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 82 | global_acc: 65.692% | global_loss: 1.1341313123703003\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 83 | global_acc: 63.158% | global_loss: 1.1318418979644775\n",
            "17/17 [==============================] - 2s 62ms/step\n",
            "comm_round: 84 | global_acc: 66.667% | global_loss: 1.126391887664795\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 85 | global_acc: 65.887% | global_loss: 1.1329600811004639\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 86 | global_acc: 65.497% | global_loss: 1.1375327110290527\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 87 | global_acc: 66.862% | global_loss: 1.1247894763946533\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 88 | global_acc: 65.692% | global_loss: 1.1247466802597046\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 89 | global_acc: 65.692% | global_loss: 1.1275475025177002\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 90 | global_acc: 65.692% | global_loss: 1.132174015045166\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 91 | global_acc: 62.573% | global_loss: 1.1238985061645508\n",
            "17/17 [==============================] - 2s 57ms/step\n",
            "comm_round: 92 | global_acc: 65.107% | global_loss: 1.125172734260559\n",
            "17/17 [==============================] - 2s 59ms/step\n",
            "comm_round: 93 | global_acc: 65.107% | global_loss: 1.1219111680984497\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 94 | global_acc: 65.692% | global_loss: 1.120580792427063\n",
            "17/17 [==============================] - 2s 58ms/step\n",
            "comm_round: 95 | global_acc: 66.082% | global_loss: 1.1255499124526978\n",
            "17/17 [==============================] - 2s 65ms/step\n",
            "comm_round: 96 | global_acc: 66.472% | global_loss: 1.1208782196044922\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 97 | global_acc: 66.082% | global_loss: 1.1275273561477661\n",
            "17/17 [==============================] - 2s 63ms/step\n",
            "comm_round: 98 | global_acc: 62.183% | global_loss: 1.1185600757598877\n",
            "17/17 [==============================] - 2s 60ms/step\n",
            "comm_round: 99 | global_acc: 65.497% | global_loss: 1.1188501119613647\n",
            "17/17 [==============================] - 2s 56ms/step\n",
            "comm_round: 1 | global_acc: 80.702% | global_loss: 0.9714491963386536\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.optimizers import legacy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from fl_implementation_utils import *\n",
        "\n",
        "# declear path to your mnist data folder\n",
        "img_path = \"/content/Alzheimer_s Dataset/train\"\n",
        "\n",
        "# get the path list using the path object\n",
        "image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "# apply our function\n",
        "image_list, label_list = load(image_paths, input_shape=(100, 100, 3), verbose=10000)\n",
        "\n",
        "# binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "label_list = lb.fit_transform(label_list)\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    image_list, label_list, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# create clients\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial=\"client\")\n",
        "\n",
        "# process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "for client_name, data in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "\n",
        "# process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
        "\n",
        "comms_round = 100\n",
        "\n",
        "# create optimizer\n",
        "lr = 0.01\n",
        "loss = \"categorical_crossentropy\"\n",
        "metrics = [\"accuracy\"]\n",
        "optimizer = legacy.SGD(lr=lr, decay=lr / comms_round, momentum=0.9)\n",
        "\n",
        "# initialize global model\n",
        "smlp_global = PretrainedInceptionModel()\n",
        "global_model = smlp_global.build(input_shape=(100, 100, 3), num_classes=4)\n",
        "\n",
        "# commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    # randomize client data - using keys\n",
        "    client_names = list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    # loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = PretrainedInceptionModel()\n",
        "        local_model = smlp_local.build(input_shape=(100, 100, 3), num_classes=4)\n",
        "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "        # set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        # fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        # scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        # clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    # to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    # test global model and print out metrics after each communications round\n",
        "    for X_test, Y_test in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
        "SGD_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    .shuffle(len(y_train))\n",
        "    .batch(320)\n",
        ")\n",
        "smlp_SGD = PretrainedInceptionModel()\n",
        "SGD_model = smlp_SGD.build(input_shape=(100, 100, 3), num_classes=4)\n",
        "\n",
        "SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
        "\n",
        "# test the SGD global model and print out metrics\n",
        "for X_test, Y_test in test_batched:\n",
        "    SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LPjbBmnVrZb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
